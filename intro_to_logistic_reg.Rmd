---
title: "Logistic Regrets 1: intro to logistic regression"
author: "Michael Chimento"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1337)
library(tidyverse)
```

## Introduction

Modeling the probability of an animal producing one behavior or another is a recurring theme in the study of animal behavior. This brief guide is intended to establish some groundwork for how to approach the analysis of behavioral choices. We will simulate some behavioral data, and learn how to run and interpret a logistic GLM.

Logistic models are trickier than a standard linear model to interpret, and running them on simulated data will allow us to see how the model captures structure in that type of data. We will further cover some basic methods for plotting the inferred values of the model parameters, in order to better understand how they would be useful in a real situation.

## Simulating data

Let's imagine an experiment that you've just designed. You have a population of great tits that all have been banded with an RFID code, so you can identify individuals. You have feeders which have antennas that read their identity, and can give or deny access to the feeder, based on the identity of the bird. You split the birds into two groups, where Group A only has access to Feeder A, and Group B only has access to Feeder B. You'd like to test how quickly the birds learn, or, how the probability of choosing the "correct" feeder changes with each visit to the feeder.

Let's start by defining a vector of identities for individual birds. We will take advantage of R's letters variable, which contains the entire lower case, English alphabet.

```{r bird_ID}
bird_ID = letters
print(bird_ID)
```

The alphabet, as you may remember, has 26 letters, so we have 26 birds. We can make a vector indicating which experimental group each bird belongs to.

```{r group}
group = c(rep_len("A",13), rep_len("B",13))
print(group)
```
Next, let's assign an age variable to the birds. This will be binary, either juvenile or adult. Let's assume we're being good scientists, and we've balanced each group by age (well, almost).

```{r age}
age = rep_len(c("juv.","ad."),26)
print(age)
```

Cool, now let's introduce our little friend, `rbinom()`. The process were interested in, of choosing correct or incorrect, can be modeled as a Bernoulli trial, in statistical speak. `rbinom()` generates series of Bernoulli trials, and it takes three arguments: 1) the number of observations we want, 2) the number of trials per observation, and 3) the probability of a successful trial. Now, for the sake of simplicity, let's assume each bird has landed on either Feeder A or Feeder B 100 times. These are very precise birds!

Let's use `rbinom()` to model 100 trials from a bird which never learns. Let's say, it's bird "a", who is also in group "A", meaning that it only has access to Feeder A. It just randomly chooses between Feeder A and Feeder B, never learning which is the right one, with $p(Feeder_A)=.5$. In the following output, 0 is a fail (visit to Feeder B), and 1 is a success (visit to Feeder A).

```{r bernoulli}
correct = rbinom(100, 1, 0.5)
table(correct)
```

Let's say it's not just this bird, but all of our birds are quite dull, or perhaps sick of our incessant experiments. They want to eat, but refuse to learn. Let's make a dataframe to hold simulated data for the entire experimental population. First, we'll need to populate the data frame with 100 rows per bird. Trial number will be indexed from 0, such that the first trial will represent the intercept in our model.

```{r df_empty}
df_empty = tibble(bird_ID,group,age) %>% 
  slice(rep(1:n(), each = 100)) %>%
  mutate(bird_ID = as.factor(bird_ID), group = as.factor(group), age = as.factor(age)) %>% 
  group_by(bird_ID) %>% 
  mutate(trial_no=c(0:99)) %>% 
  ungroup()
summary(df_empty)
```
Next, let's group by each bird, and add 2 variables, one is the trial number, and the other is whether the choice was correct or incorrect,  generated by `rbinom()`.

```{r df_dumb}
df_dumb = df_empty %>% 
  group_by(bird_ID) %>%
  mutate(correct = rbinom(n(),1,0.5))
summary(df_dumb)
df_dumb %>% filter(bird_ID=="a") %>% head()
```

Ok, so this looks like how we expect it. Importantly, the data is in "long" format, where each row is 1 observation, and the ILVs of each bird is repeated with every observation.

The variable `correct` ranges from 0 to 1, with an average of .49, which is pretty darn close to the .5 probability we set for these birds to choose. When all of this data (2600 observations) is pooled together, our sample size seems large enough to accurately recover the parameter value from the mean. But what if we look at individual birds separately? Let's take a peek.

```{r}
df_dumb %>% group_by(bird_ID) %>% summarize(mean=mean(correct),sd=sd(correct))
```

We have a bit more variation within birds due to sampling error. Here, we know exactly what process generated the data, and that the birds are normally distributed around 0.5. However, if it were real data, we wouldn't be so sure if variation was due to sampling error, or reflected real differences between the birds. Of course, it will likely be both. We have to infer the parameters of the process that generated the real data. To infer parameters of the relationship between variables and Bernoulli trials, we can use **logistic regression**.

### Digression: probability vs. Odds vs. Log odds

To understand how logistic regression works, we need to see the relationship between probabilities (range from 0 to 1), odds (range from 0 to infinity), and log odds (range from negative to positive infinity). Let's make a table to see the relationship.

```{r prob_odds_LO}
probability = seq(0,1,.1)
odds = probability/(1-probability)
log_odds = log(odds)
df_relationship = tibble(probability,odds,log_odds)
knitr::kable(df_relationship)
```

Logistic regressions take advantage of the fact that probabilities can be represented by linear combinations of log odds. The response of probability to predictors can be modeled by this underlying linear relationship. The mathematical formulas that explain the relationship between the predictors and response is:

1. The relationship between log odds and the predictors:
$$
ln(\frac{p(y)}{1-p(y)})=\beta_0+\beta_1x_1 + ... \beta_nx_n
$$
2. Which is equivalent to this:
$$
p(y)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1 + ... \beta_nx_n)}}
$$

After the algebra to solve for $p(y)$, the usual linear model is now tucked away in the denominator. The linear relationship this describes will be translated to a non-linear relationship of probability. We will see later that logistic regressions fall apart, the same way a regular linear regression does, when the relationship between our predictor and the log odds is not linear. For now, we are simulating data, so we can ensure that linear relationship!

Back to our population of dumb birds, in which we know that there is **no** relationship between trial number and the probability of success.

```{r}
m1 = glm(correct ~ 1 + trial_no, data=df_dumb, family=binomial)
summary(m1)
```

Beta estimates are given in log odds, here. To see what that means in terms of probability, we can convert these back into probability using our handy table above. The intercept value is about $0$, or $p(correct)=0.5$, as we expected. Further, we find that the estimate for trial number is roughly $0$, and there is no significant effect of trial number. The birds never learned, and the probability of choosing the correct choice remains $0.5$ independent of how many times a bird visited the feeder. However, using this table is cumbersome, and it's easier, and more accurate to write a function that converts these numbers for us. Let's do that now, since we'll use it in a moment

```{r}
log_to_prob <- function(log_odds){
  odds <- exp(log_odds)
  prob <- odds / (1 + odds)
  return(prob)
}

prob_to_log <- function(prob){
  odds <- prob/(1-prob)
  log_odds <- log(odds)
  return(log_odds)
}

```

So, what if we introduce a positive, linear relationship between trial and correct? We already have the variable in our dataframe, so all we need to do is put it into the `rbinom` function that produces $p(correct)$. The intercept (which represents the first trial) should be 0 log odds (1:1 odds, or .5 probability), since we don't expect birds to prefer the "wrong" feeder with no experience.

```{r df_smart_linear_logit}
df_smart = df_empty %>% 
  rowwise() %>% 
  mutate(correct = rbinom(1,1,log_to_prob(.1*trial_no)))
summary(df_smart)

ggplot(df_smart,aes(x=trial_no, y=correct))+
  geom_jitter(aes(color=bird_ID),height=0.1,show.legend = F,alpha=0.5)+
  stat_smooth()
```

GGplot's `stat_smooth()` function draws a best fitting GAM onto the data. This is fine for casually eye-balling relationships, but is not sufficient for anything worthy of publication. GAMs are pretty complicated, and GGPlot does everything under the hood---recovering parameter values is not so straightforward. Let's look at it with logistic regression.

```{r glm_smart_linear}
m1 = glm(correct ~ 1 + trial_no, data=df_smart, family=binomial)
summary(m1)
```

Our relationship between trial number and success is simple here. To interpret the probability estimated at the intercept, we can put it in our helper function. To interpret the beta for trial number, we can simply exponentiate.

```{r}
log_to_prob(coef(m1)[1])
exp(coef(m1)[2])
```

The coefficient for trial number indicates the difference in log odds per 1 trial. Exponentiating this difference gives us the odds. Odds over 1 indicate a positive relationship, with decimal following one being interpreted as the percent increase in the odds of choosing correctly per trial (here, approximate 10% more likely to choose correct per trial). To recover the specific probability of choosing the correct feeder at a given trial, we can plug in values.

```{r}
log_to_prob(coef(m1)[1] + coef(m1)[2]*1)
log_to_prob(coef(m1)[1] + coef(m1)[2]*9)
```
On the second trial, the probability of choosing the correct choice is $.53$. On the tenth trial, it's $.72$.

Let's plot the predicted values from the model over the real values.
```{r}
df_smart$preds = predict(m1,type="response")
ggplot(df_smart,aes(x=trial_no,y=correct))+
  geom_jitter(aes(color=bird_ID),height=0.1,show.legend = F,alpha=0.5)+
  stat_summary(aes(x=trial_no,y=correct),color="darkblue",alpha=0.5)+
  stat_summary(aes(x=trial_no,y=preds))+
  labs(x="trial number",y="correct feeder",title="Group A")
```

The fitted mean probability, conditional on trial number, is shown over the real mean. This looks pretty close to the GAM we saw before, and fits the data reasonably well.

One really important point here, is that the relationship we just modeled is **totally different** from specifying a linear relationship directly in the changes in probability. Just for fun, let's do that now, and see what we get. We have to bit a bit careful how we implement this relationship, since probability can't exceed 1. Let's again assume the intercept should be .5. Let's increment the probability by 1/200th every trial, so that at 100 trials, the birds should be choosing the correct feeder all the time.
```{r df_smart_linear_prob}
df_smart = df_empty %>% 
  rowwise() %>% 
  mutate(correct = rbinom(1,1,.5+.005*trial_no))
summary(df_smart)

ggplot(df_smart,aes(x=trial_no, y=correct))+
  geom_point()+
  stat_smooth()
```


We're directly manipulating probability here, not the log odds. Let's see what this probability function looks like in log-odds.

```{r}
prob_seq=seq(0.5,0.999,0.005)
LO_seq=prob_to_log(prob_seq)
qplot(x=c(0:99),y=LO_seq)
```

Weird, right? Such a simple linear increase in probability ends up with a non-linear increase in log-odds. Let's see how a logistic model likes it.

```{r glm_smart_linear_prob}
m1 = glm(correct ~ 1 + trial_no, data=df_smart, family=binomial)
summary(m1)
log_to_prob(coef(m1)[1])
```

```{r}
df_smart$preds = predict(m1,type="response")
ggplot(df_smart,aes(x=trial_no,y=correct))+
  geom_jitter(aes(color=bird_ID),height=0.1,show.legend = F,alpha=0.5)+
  stat_summary(aes(x=trial_no,y=correct),color="darkblue",alpha=0.5)+
  stat_summary(aes(x=trial_no,y=preds))+
  labs(x="trial number",y="correct feeder",title="Group A")
```

By trying to fit a straight line through this curve, it's ended up underestimating the intercept, and it will have over estimated the probabilities in the middle of the trial sequence. The AIC score is higher on this model than the previous one, indicating a weaker fit to the data. The distinction between the relationship between log odds and predictors, versus a direct relationship between probability and the predictors is really important for understanding how these models work!